#GEN AI basics
#AIzaSyDgfPuTLEAvrkEMcNirpuj2FMQGipBIt7s

- Topics
. Gnerative AI--generate new data based on training sample,it generate image,test,audio,videos etc data as output.it classify into two like Generative Image and Language Models.
-- AI -->ML--> Deep Learning --> Gnerative AI ML is subset of AI.

. Large Language Models (LLMs)
. Open AI
. LangChain
. Vector Database
. Llama Index
. Open source LLM model
. End to End Project


# ChartGPT-- Open AI
# Google Bard from google
# Meta Llama 2 from facebook

## Deep Learning--types of neural N/W
1) ANN--artificial neural n/w
2) CNN-- convolution 
3) RNN--recurrent-- used where sequence related data
4) RL
5) GAN

#What are foundation models in Generative AI?
Fountadtion model is large AI model which pretrained on vast amount/qualtity of data that was designed to adaptive(or fine tuned) to wide variety/range of downstream tasks,sush sentimental analysis, image captioning, and object recognition.

#Hallucinations are words or phrases that are generated by the model that are often nonsensical or grammatically incorrect. What are some factors that can cause hallucinations?
- model is not trained on enough data.
- model is trained on incorrect of noisy/dirty data
- model is not given enough context\
- model is having proper constraints
- three main concerns on large language models, hallucinations, factuality, and anthropomorphization.
    - AI model generates content that is unrealistic, fictional, or completely fabricated.Factuality relates to the accuracy or truthfulness of the information generated
      Anthropomorphization refers to the attribution of human-like qualities, characteristics, or behaviors to non-human entities, such as machines or AI models.
      The key benefit of ethical practices in an organization is that they can help to avoid bringing harm to customers, users, and society at large.

## AI Principles--responsible use of AI
- Ai should be socially beneficial.
- Avoid creating or reinforcing unfair bias--race gender,slaray,sexual
- Be built and test for safety
- Be accountable to people
- incorporate privacy designe principles.like transparency.
- Uphold high standards of scientific excellence.-share more docs
- Be made available for uses that accord with these principles.

- Those AI applications that are likely to cause overall harm, weapons or other technologies whose principal purpose is to
- cause injury to people, surveillance violating internationally accepted norms, and those whose purpose contravenes international law and human rights.
- These seven aims and four areas together make up Googleâ€™s AI principles and succinctly communicate our values in developing advanced technologies.
- Once you have your AI principles defined, the next step in establishing AI governance is to set up a review process to put them into practice.AI governance process allowed us to research and scope a product that aligned with our AI principles.

## What Problems Model Solve?
- who are the intended users?
- what other groups may be impcated?
- what groups are invisible today?
- how awas the training data collected,sampled and labelled?
- is the training data skewed?
- how was the model tested and validated?
- is the model behaving as expected?


# Large Language Models- refer to large,general purpose language models that can be pre-trained and then fine-tuned for specific purposes
- are trained to solve common language problem-text classification,text generation,question answering and document summarization
--Large - training datasts, large number of parameters

# Google release Palm in april 22
- PaLM 2- Pathways Language Model
    - it has 540 billion parameters
    - large training dataset
    - large number of parameters
    Leverage the new pathways systems
    commonality of human languages
    -resource restriction


# Promp Design is the proces of creating a prompt that is tailored to the specific taks that the system is being asked to perform.
# Prompt engineering is the process of creating a prompt that is designed to improve performance.

# More effiecient methods of tuning a model
- Parameter-efficient tuning methods (PETM) -methods for tuning an LLM on your own customer data with duplicating the model.
- Prompt tuning--one of the easiest parameter-efficient tuning methods
https://medium.com/@fahadp2000/cracking-the-code-machine-learning-for-next-word-predictions-8dfe9387cc27
https://www.cloudskillsboost.google/paths/118/course_templates/976/labs/466368

# Prompt engineering best practices
Prompt engineering is all about how to design your prompts so that the response is what you were indeed hoping to see.

The idea of using "unfancy" prompts is to minimize the noise in your prompt to reduce the possibility of the LLM misinterpreting the intent of the prompt. Below are a few guidelines on how to engineer "unfancy" prompts.

In this section, you'll cover the following best practices when engineering prompts:

Be concise-Recommended. The prompt below is to the point and concise.
Be specific, and well-defined
Ask one task at a time
Improve response quality by including examples
Turn generative tasks to classification tasks to improve safety.

# hallucinations
Although LLMs have been trained on a large amount of data, they can generate text containing statements not grounded in truth or reality; these responses from the LLM are often referred to as "hallucinations" due to their limited memorization capabilities. Note that simply prompting the LLM to provide a citation isnâ€™t a fix to this problem, as there are instances of LLMs providing false or inaccurate citations. Dealing with hallucinations is a fundamental challenge of LLMs and an ongoing research area, so it is important to be cognizant that LLMs may seem to give you confident, correct-sounding statements that are in fact incorrect.

Note that if you intend to use LLMs for the creative use cases, hallucinating could actually be quite useful.
since no elephant has ever flown to the moon. But how do we prevent these kinds of inappropriate questions and more specifically, reduce hallucinations?
There is one possible method called the Determine Appropriate Response (DARE) prompt, which cleverly uses the LLM itself to decide whether it should answer a question based on what its mission is.

- Classification tasks reduces output variability
- Improve response quality by including examples
 - example of zero-shot prompting, where you don't provide any examples to the LLM within the prompt itself.
``` prompt = """Decide whether a Tweet's sentiment is positive, neutral, or negative.
Tweet: I loved the new YouTube video you made!
Sentiment:
"""
print(generation_model.predict(prompt=prompt, max_output_tokens=256).text)
```
 - example of one-shot prompting, where you provide one example to the LLM within the prompt to give some guidance on what type of response you want.
 ``` prompt = """Decide whether a Tweet's sentiment is positive, neutral, or negative.
Tweet: I loved the new YouTube video you made!
Sentiment: positive
Tweet: That was awful. Super boring ðŸ˜ 
Sentiment:
"""
print(generation_model.predict(prompt=prompt, max_output_tokens=256).text) ```

- example of few-shot prompting, where you provide a few examples to the LLM within the prompt to give some guidance on what type of response you want.
``` 
prompt = """Decide whether a Tweet's sentiment is positive, neutral, or negative.
Tweet: I loved the new YouTube video you made!
Sentiment: positive
Tweet: That was awful. Super boring ðŸ˜ 
Sentiment: negative
Tweet: Something surprised me about this video - it was actually original. It was not the same old recycled stuff that I always see. Watch it - you will not regret it.
Sentiment:
"""
print(generation_model.predict(prompt=prompt, max_output_tokens=256).text)

```
The zero-shot prompts are more open-ended and can give you creative answers, while one-shot and few-shot prompts teach the model how to behave so you can get more predictable answers that are consistent with the examples provided